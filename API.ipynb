{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import psycopg2\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from typing import Dict, Tuple, Union\n",
    "from langchain_core.structured_query import (\n",
    "    Comparator,\n",
    "    Comparison,\n",
    "    Operation,\n",
    "    Operator,\n",
    "    StructuredQuery,\n",
    "    Visitor,\n",
    ")\n",
    "from langchain_core.tools import InjectedToolArg, tool\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from copy import deepcopy\n",
    "from langchain_core.runnables import chain\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from psycopg import Connection\n",
    "import uuid\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"my_docs2\",\n",
    "    connection=\"postgresql+psycopg://stefan:gigelfrone112@localhost:5432/techvector\",\n",
    ")\n",
    "app = FastAPI()\n",
    "conn = psycopg2.connect(\"dbname=techvector user=stefan password=gigelfrone112 host=localhost port=5432\")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_date_objects(data):\n",
    "    \"\"\"\n",
    "    Recursively traverses the JSON object and replaces every dictionary\n",
    "    containing 'date' and 'type' keys with the value of the 'date' key.\n",
    "\n",
    "    :param data: JSON object (dict, list, or other types)\n",
    "    :return: Updated JSON object\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        # Check if the current dictionary is the one to replace\n",
    "        if \"date\" in data and \"type\" in data:\n",
    "            return data[\"date\"]\n",
    "        # Otherwise, process each key-value pair\n",
    "        return {key: replace_date_objects(value) for key, value in data.items()}\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        # Process each element in the list\n",
    "        return [replace_date_objects(item) for item in data]\n",
    "\n",
    "    # Return the data as is for other types\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTranslator(Visitor):\n",
    "    \"\"\"Translate `PGVector` internal query language elements to valid filters.\"\"\"\n",
    "\n",
    "    allowed_operators = [Operator.AND, Operator.OR]\n",
    "    \"\"\"Subset of allowed logical operators.\"\"\"\n",
    "    allowed_comparators = [\n",
    "        Comparator.EQ,\n",
    "        Comparator.NE,\n",
    "        Comparator.GT,\n",
    "        Comparator.LT,\n",
    "        Comparator.IN,\n",
    "        Comparator.NIN,\n",
    "        Comparator.CONTAIN,\n",
    "        Comparator.LIKE,\n",
    "    ]\n",
    "    \"\"\"Subset of allowed logical comparators.\"\"\"\n",
    "\n",
    "    def _format_func(self, func: Union[Operator, Comparator]) -> str:\n",
    "        self._validate_func(func)\n",
    "        return f\"${func.value}\"\n",
    "\n",
    "    def visit_operation(self, operation: Operation) -> Dict:\n",
    "        args = [arg.accept(self) for arg in operation.arguments]\n",
    "        return {self._format_func(operation.operator): args}\n",
    "\n",
    "\n",
    "\n",
    "    def visit_comparison(self, comparison: Comparison) -> Dict:\n",
    "        return {\n",
    "            comparison.attribute: {\n",
    "                self._format_func(comparison.comparator): comparison.value\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def visit_structured_query(\n",
    "        self, structured_query: StructuredQuery\n",
    "    ) -> Tuple[str, dict]:\n",
    "        if structured_query.filter is None:\n",
    "            kwargs = {}\n",
    "        else:\n",
    "            kwargs = {\"filter\": structured_query.filter.accept(self)}\n",
    "            kwargs = replace_date_objects(kwargs)\n",
    "        return structured_query.query, kwargs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The title that the article was published under\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"author\",\n",
    "        description=\"The name of the author of the article\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"date\",\n",
    "        description=\"The date that the article was published on, in the format 'YYYY-MM-DD'. If the month is given by its name, it is converted to its number.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"category\",\n",
    "        description=\"The category that the article belongs to. One of ['AI', 'Apps', 'Biotech & Health', 'Climate', 'Commerce', 'Crypto', 'Enterprise', 'Fintech', 'Fundraising', 'Gadgets', 'Gaming', 'Government & Policy', 'Hardware', 'Media & Entertainment', 'Privacy', 'Robotics', 'Security', 'Social', 'Space', 'Startups', 'Transportation', 'Venture']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"url\",\n",
    "        description=\"The URL to the original TechCrunch article\",\n",
    "        type=\"link\",\n",
    "    )\n",
    "]\n",
    "document_content_description = \"The article content\"\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vector_store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    structured_query_translator=CustomTranslator(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/get_articles_by_query\")\n",
    "async def get_articles_by_query(query: str):\n",
    "    query = query.replace(\"'\", \"\\'\")\n",
    "    query = query.replace(\"’\", \"\\'\")\n",
    "    docs = retriever.invoke(query)\n",
    "    urls = list(set([doc.metadata[\"url\"] for doc in docs]))\n",
    "    cursor.execute(\"SELECT * FROM article WHERE link = ANY(%s);\", (urls,))\n",
    "    tuples = cursor.fetchall()\n",
    "    result_dict = [dict(zip(['url', 'title', 'time', 'img', 'category', 'summary', 'questions', 'author'], tup)) for tup in tuples]\n",
    "    return result_dict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/get_articles\")\n",
    "async def get_articles():\n",
    "    cursor.execute(\"SELECT * FROM article;\")\n",
    "    tuples = cursor.fetchall()\n",
    "    result_dict = [dict(zip(['url', 'title', 'time', 'img', 'category', 'summary', 'questions', 'author'], tup)) for tup in tuples]\n",
    "    return result_dict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/get_article\")\n",
    "async def get_article(url: str):\n",
    "    cursor.execute(f\"SELECT * FROM article where link = '{url}';\")\n",
    "    tuples = cursor.fetchone()\n",
    "    result_dict = dict(zip(['url', 'title', 'time', 'img', 'category', 'summary', 'questions', 'author'], tuples))\n",
    "    return result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://techcrunch.com/2025/01/09/nvidias-ai-avatar-sat-on-my-computer-screen-and-weirded-me-out/', 'title': 'Nvidia’s AI avatar sat on my computer screen and weirded me out', 'time': '4:31 PM PST · January 9, 2025', 'img': 'https://techcrunch.com/wp-content/uploads/2025/01/IMG_9CC69B31B7BF-1.jpeg?w=1024', 'category': 'AI', 'summary': 'Nvidia has introduced R2X, a prototype AI avatar designed to assist users directly from their desktop, combining advanced AI models with a human-like interface. While it can navigate apps, process files, and even observe users’ screens, early demos reveal some quirks, like odd facial expressions and occasional inaccuracies in its guidance. The company plans to open source R2X in 2025, potentially allowing developers to create personalized AI interactions. Despite its promise, the technology still faces challenges, hinting at both the excitement and the uncertainties of integrating AI into everyday computing.', 'questions': 'What specific features does R2X offer for assisting users with applications?&&&How does Nvidia plan to address the issues observed in the early demos of R2X?&&&What are the implications of Nvidia’s plans to open source the R2X avatars for developers and users?', 'author': 'Maxwell Zeff'}]\n"
     ]
    }
   ],
   "source": [
    "print(await get_articles_by_query('Give me an article about AI published on 9 January, 2025 by Maxwell Zeff, talking about nvidia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer based on the retrieved context,\"\n",
    "        \"say that the context doesn't contain the answer, but nevertheless try to provide an\"\n",
    "        \"explanation based on your pre-trained knowledge. If you still don't know,\"\n",
    "        \"say that you don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.It is ABSOLUTELY NECESSARY to mention that the retrieved context does not contain the answer if it does not.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "db_url = \"postgresql://stefan:gigelfrone112@localhost:5432/techvector\"\n",
    "\n",
    "postgresCheckpointer = PostgresSaver(Connection.connect(db_url))\n",
    "\n",
    "#postgresCheckpointer.setup()\n",
    "graph = graph_builder.compile(checkpointer=postgresCheckpointer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class chatbot_data(BaseModel):\n",
    "    query: str\n",
    "    thread_id: str = \"\"\n",
    "\n",
    "@app.post(\"/general_chatbot\")\n",
    "async def general_chatbot(chatbot_data: chatbot_data):\n",
    "    query = chatbot_data.query\n",
    "    thread_id = chatbot_data.thread_id\n",
    "\n",
    "    if thread_id == \"\":\n",
    "        thread_id = str(uuid.uuid4())\n",
    "\n",
    "    input_message = query\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    ans = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": input_message}]} ,config=config,)\n",
    "    \n",
    "    return ans[\"messages\"][-1], thread_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AIMessage(content='The RTX 5090 GPU is claimed to outperform the RTX 4090 by as much as 2x. It features 92 billion transistors, 4,000 AT TOPS, 380 ray-tracing TFLOPS, and 1.8 TB/s bandwidth. Additionally, it delivers breakthroughs in AI-driven rendering, including neural shaders and enhanced geometry and lighting capabilities.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 924, 'total_tokens': 1002, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-48f36081-e927-4647-bbf6-d3b4141fc391-0', usage_metadata={'input_tokens': 924, 'output_tokens': 78, 'total_tokens': 1002, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), '7964dd41-14b6-4b6e-8478-63a9ae58de86')\n"
     ]
    }
   ],
   "source": [
    "print(await general_chatbot(\"What are some improvements RTX 5090 GPU has over its predecessor?\", \"7964dd41-14b6-4b6e-8478-63a9ae58de86\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CustomState(MessagesState):\n",
    "    url: str\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_by_url(query: str, url: Annotated[str, InjectedToolArg]) -> Tuple[str, list]:\n",
    "    \"\"\"Retrieve information related to a query, only fetching documents with a specific URL.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2, filter={\"url\": {'$eq': url}})\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond_custom(state: CustomState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve_by_url])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    for call in response.tool_calls:\n",
    "        call[\"args\"][\"url\"] = state['url']\n",
    "\n",
    "\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools_by_url = ToolNode([retrieve_by_url])\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate_custom(state: CustomState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer based on the retrieved context,\"\n",
    "        \"PLEASE EXPLICITLY SAY that the context doesn't contain the answer, but nevertheless try to provide an\"\n",
    "        \"explanation based on your pre-trained knowledge. If you still don't know,\"\n",
    "        \"say that you don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise. It is ABSOLUTELY NECESSARY to mention that the retrieved context does not contain the answer if it does not.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(CustomState)\n",
    "\n",
    "workflow.add_node(tools_by_url)\n",
    "workflow.add_node(query_or_respond_custom)\n",
    "workflow.add_node(generate_custom)\n",
    "\n",
    "workflow.set_entry_point(\"query_or_respond_custom\")\n",
    "workflow.add_edge(\"tools\", \"generate_custom\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"query_or_respond_custom\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate_custom\", END)\n",
    "\n",
    "db_url = \"postgresql://stefan:gigelfrone112@localhost:5432/techvector\"\n",
    "\n",
    "postgresCheckpointer = PostgresSaver(Connection.connect(db_url))\n",
    "\n",
    "postgresCheckpointer.setup()\n",
    "url_graph = workflow.compile(checkpointer=postgresCheckpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What did I ask you first?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked, \"What did I ask you first?\"\n"
     ]
    }
   ],
   "source": [
    "input_message = \"What did I ask you first?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "for step in url_graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}], \"url\": \"https://techcrunch.com/2025/01/09/innovaccer-aims-to-become-healthcares-ai-powerhouse-with-275m-series-f/\"},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Url_chatbot_data(BaseModel):\n",
    "    query: str\n",
    "    url: str\n",
    "    thread_id: str = \"\"\n",
    "\n",
    "@app.post(\"/url_chatbot\")\n",
    "async def url_chatbot(url_chatbot_data: Url_chatbot_data):\n",
    "    query = url_chatbot_data.query\n",
    "    url = url_chatbot_data.url\n",
    "    thread_id = url_chatbot_data.thread_id\n",
    "\n",
    "    if thread_id == \"\":\n",
    "        thread_id = str(uuid.uuid4())\n",
    "\n",
    "    input_message = query\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    ans = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": input_message}], \"url\": url}, config=config)\n",
    "    \n",
    "    return ans[\"messages\"][-1], thread_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AIMessage(content='The context does not contain the answer to whether the idea behind Facebook was stolen. However, based on pre-trained knowledge, the founding of Facebook involved legal disputes, particularly with the Winklevoss twins, who claimed that Mark Zuckerberg stole their idea for a social networking site. Ultimately, the case was settled, and Zuckerberg has maintained that he developed Facebook independently.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 883, 'total_tokens': 955, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-35d8181d-6b58-4b6d-9101-403c4f3dd82a-0', usage_metadata={'input_tokens': 883, 'output_tokens': 72, 'total_tokens': 955, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), 'f27368c9-b02b-49d8-8902-be486010682d')\n"
     ]
    }
   ],
   "source": [
    "print(await url_chatbot(Url_chatbot_data(query=\"Was the idea behind Facebook stolen?\", url=\"https://techcrunch.com/2025/01/09/google-searches-for-deleting-facebook-instagram-explode-after-meta-ends-fact-checking/\", thread_id=\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v': 1, 'id': '1efd71f5-7d29-6990-8001-84a197576a55', 'ts': '2025-01-20T11:11:48.915226+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.3561542597271431'}, 'query_or_respond_custom': {'start:query_or_respond_custom': '00000000000000000000000000000002.0.005184707796906829'}}, 'channel_versions': {'url': '00000000000000000000000000000002.0.44641456651518807', 'messages': '00000000000000000000000000000003.0.9933475634978586', '__start__': '00000000000000000000000000000002.0.12023397766813171', 'query_or_respond_custom': '00000000000000000000000000000003.0.6729835461101962', 'start:query_or_respond_custom': '00000000000000000000000000000003.0.29169407040297723'}, 'channel_values': {'url': 'https://techcrunch.com/2025/01/09/innovaccer-aims-to-become-healthcares-ai-powerhouse-with-275m-series-f/', 'messages': [HumanMessage(content='What did I ask you first?', additional_kwargs={}, response_metadata={}, id='ca6b6716-0a63-4268-840e-2522d2d939af'), AIMessage(content='You asked, \"What did I ask you first?\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 60, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-43278417-4f75-4a3b-946b-c2bb06ed0a24-0', usage_metadata={'input_tokens': 60, 'output_tokens': 13, 'total_tokens': 73, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'query_or_respond_custom': 'query_or_respond_custom'}}\n"
     ]
    }
   ],
   "source": [
    "# from langgraph_sdk import get_client\n",
    "\n",
    "# client = get_client()\n",
    "\n",
    "# specific_thread = await client.threads.get('5')\n",
    "\n",
    "# print(specific_thread)\n",
    "\n",
    "print(postgresCheckpointer.get({\"configurable\": {\"thread_id\": \"1\"}}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
