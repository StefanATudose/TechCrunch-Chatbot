{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import psycopg2\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings \n",
    "from langchain_postgres import PGVector\n",
    "from typing import Dict, Tuple, Union, Annotated, List\n",
    "from langchain_core.structured_query import (\n",
    "    Comparator,\n",
    "    Comparison,\n",
    "    Operation,\n",
    "    Operator,\n",
    "    StructuredQuery,\n",
    "    Visitor,\n",
    ")\n",
    "from langchain_core.tools import InjectedToolArg, tool\n",
    "from langgraph.graph import MessagesState, StateGraph, END\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from psycopg import Connection\n",
    "import uuid\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import unquote\n",
    "from langchain_core.documents import Document\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "\n",
    "#####Section 1: Global Framework Variables Definition\n",
    "\n",
    "load_dotenv()\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],  # Adjust this to specify allowed origins, e.g., [\"http://localhost:3000\"]\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  # You can specify methods like [\"GET\", \"POST\"]\n",
    "    allow_headers=[\"*\"],  # You can specify allowed headers like [\"Content-Type\", \"Authorization\"]\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"my_docs2\",\n",
    "    connection=\"postgresql+psycopg://stefan:gigelfrone112@localhost:5432/techvector\",\n",
    ")\n",
    "\n",
    "conn = psycopg2.connect(\"dbname=techvector user=stefan password=gigelfrone112 host=localhost port=5432\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "db_url = \"postgresql://stefan:gigelfrone112@localhost:5432/techvector\"\n",
    "postgresCheckpointer = PostgresSaver(Connection.connect(db_url))\n",
    "#postgresCheckpointer.setup()  #first time call only\n",
    "\n",
    "items_per_page = 10       # for main page pagination\n",
    "\n",
    "#####Section 2: Building the document retriever for get_articles_by_query_api\n",
    "\n",
    "def replace_date_objects(data):\n",
    "    \"\"\"\n",
    "    Recursively traverses the JSON object and replaces every dictionary\n",
    "    containing 'date' and 'type' keys with the value of the 'date' key.\n",
    "\n",
    "    :param data: JSON object (dict, list, or other types)\n",
    "    :return: Updated JSON object\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        # Check if the current dictionary is the one to replace\n",
    "        if \"date\" in data and \"type\" in data:\n",
    "            return data[\"date\"]\n",
    "        \n",
    "        # Otherwise, process each key-value pair\n",
    "        return {key: replace_date_objects(value) for key, value in data.items()}\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        # Process each element in the list\n",
    "        return [replace_date_objects(item) for item in data]\n",
    "\n",
    "    # Return the data as is for other types\n",
    "    return data\n",
    "\n",
    "\n",
    "class CustomTranslator(Visitor):\n",
    "    \"\"\"Translate `PGVector` internal query language elements to valid filters.\"\"\"\n",
    "\n",
    "    \"\"\"Subset of allowed logical operators and comparators.\"\"\"\n",
    "    allowed_operators = [Operator.AND, Operator.OR]\n",
    "    allowed_comparators = [\n",
    "        Comparator.EQ,\n",
    "        Comparator.NE,\n",
    "        Comparator.GT,\n",
    "        Comparator.LT,\n",
    "        Comparator.IN,\n",
    "        Comparator.NIN,\n",
    "        Comparator.CONTAIN,\n",
    "        Comparator.LIKE,\n",
    "    ]\n",
    "\n",
    "    #Unchanged from official PGTranslator implementation\n",
    "    def _format_func(self, func: Union[Operator, Comparator]) -> str:\n",
    "        self._validate_func(func)\n",
    "        return f\"${func.value}\"\n",
    "\n",
    "    #Unchanged from official PGTranslator implementation\n",
    "    def visit_operation(self, operation: Operation) -> Dict:\n",
    "        args = [arg.accept(self) for arg in operation.arguments]\n",
    "        return {self._format_func(operation.operator): args}\n",
    "\n",
    "\n",
    "    #Unchanged from official PGTranslator implementation\n",
    "    def visit_comparison(self, comparison: Comparison) -> Dict:\n",
    "        return {\n",
    "            comparison.attribute: {\n",
    "                self._format_func(comparison.comparator): comparison.value\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def visit_structured_query(\n",
    "        self, structured_query: StructuredQuery\n",
    "    ) -> Tuple[str, dict]:\n",
    "        if structured_query.filter is None:\n",
    "            kwargs = {}\n",
    "        else:\n",
    "            kwargs = {\"filter\": structured_query.filter.accept(self)}\n",
    "            \n",
    "            #Reformatted \"data\" fields to be compatible with the Documents' metadata\n",
    "            kwargs = replace_date_objects(kwargs)\n",
    "        return structured_query.query, kwargs\n",
    "\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The title that the article was published under\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"author\",\n",
    "        description=\"The name of the author of the article\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"date\",\n",
    "        description=\"The date that the article was published on, in the format 'YYYY-MM-DD'. If the month is given by its name, it is converted to its number.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"category\",\n",
    "        description=\"The category that the article belongs to. One of ['AI', 'Apps', 'Biotech & Health', 'Climate', 'Commerce', 'Crypto', 'Enterprise', 'Fintech', 'Fundraising', 'Gadgets', 'Gaming', 'Government & Policy', 'Hardware', 'Media & Entertainment', 'Privacy', 'Robotics', 'Security', 'Social', 'Space', 'Startups', 'Transportation', 'Venture']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"url\",\n",
    "        description=\"The URL to the original TechCrunch article\",\n",
    "        type=\"link\",\n",
    "    )\n",
    "]\n",
    "document_content_description = \"The article content\"\n",
    "\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vector_store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    structured_query_translator=CustomTranslator(),\n",
    ")\n",
    "\n",
    "#####Section 3: Building the general_chatbot\n",
    "\n",
    "\n",
    "class StateWithArtifacts(MessagesState):\n",
    "    artifacts: List[Tuple[str, str]]\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    retrieved_docs = [(doc.metadata['title'], doc.metadata['url']) for doc in retrieved_docs]\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "\n",
    "def query_or_respond(state: StateWithArtifacts):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "def generate(state: StateWithArtifacts):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    artifacts = [message.artifact for message in tool_messages if message.artifact]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer based on the retrieved context,\"\n",
    "        \"say that the context doesn't contain the answer, but nevertheless try to provide an\"\n",
    "        \"explanation based on your pre-trained knowledge. If you still don't know,\"\n",
    "        \"say that you don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.It is ABSOLUTELY NECESSARY to mention that the retrieved context does not contain the answer if it does not.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response], \"artifacts\": artifacts}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(StateWithArtifacts)\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=postgresCheckpointer)\n",
    "\n",
    "\n",
    "##### Section 4: Building the url_chatbot\n",
    "\n",
    "\n",
    "class CustomState(MessagesState):\n",
    "    url: str\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_by_url(query: str, url: Annotated[str, InjectedToolArg]) -> Tuple[str, list]:\n",
    "    \"\"\"Retrieve information related to a query, only fetching documents with a specific URL.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2, filter={\"url\": {'$eq': url}})\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "\n",
    "    return serialized\n",
    "\n",
    "\n",
    "def query_or_respond_custom(state: CustomState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve_by_url])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    for call in response.tool_calls:\n",
    "        call[\"args\"][\"url\"] = state['url']\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "tools_by_url = ToolNode([retrieve_by_url])\n",
    "\n",
    "\n",
    "def generate_custom(state: CustomState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer based on the retrieved context,\"\n",
    "        \"PLEASE EXPLICITLY SAY that the context doesn't contain the answer, but nevertheless try to provide an\"\n",
    "        \"explanation based on your pre-trained knowledge. If you still don't know,\"\n",
    "        \"say that you don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise. It is ABSOLUTELY NECESSARY to mention that the retrieved context does not contain the answer if it does not.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(CustomState)\n",
    "\n",
    "workflow.add_node(tools_by_url)\n",
    "workflow.add_node(query_or_respond_custom)\n",
    "workflow.add_node(generate_custom)\n",
    "\n",
    "workflow.set_entry_point(\"query_or_respond_custom\")\n",
    "workflow.add_edge(\"tools\", \"generate_custom\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"query_or_respond_custom\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate_custom\", END)\n",
    "\n",
    "\n",
    "url_graph = workflow.compile(checkpointer=postgresCheckpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/conversation_history/{type}/{thread_id}\")\n",
    "async def conversation_history(type, thread_id):\n",
    "    if type == \"0\":\n",
    "        snapshot = url_graph.get_state({\"configurable\": {\"thread_id\": thread_id}})\n",
    "    elif type == \"1\":\n",
    "        snapshot = graph.get_state({\"configurable\": {\"thread_id\": thread_id}})\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    \n",
    "    # result = []\n",
    "    # for item in dir(snapshot):\n",
    "    #     attr = getattr(snapshot, item)\n",
    "    #     if callable(attr):\n",
    "    #         result.append(f\"{item}: Method\")\n",
    "    #     else:\n",
    "    #         result.append(f\"{item}: Attribute\")\n",
    "\n",
    "    \n",
    "\n",
    "    messages = snapshot.values[\"messages\"]\n",
    "    \n",
    "    conversation_messages = [\n",
    "        message.content\n",
    "        for message in messages\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "\n",
    "    return conversation_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def url_chatbot(query, url, thread_id):\n",
    "\n",
    "    if thread_id == \"\":\n",
    "        thread_id = str(uuid.uuid4())\n",
    "\n",
    "    input_message = query\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    ans = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": input_message}], \"url\": url}, config=config)\n",
    "    \n",
    "    return ans[\"messages\"][-1], thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 48, 'total_tokens': 60, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-08b7c61e-365c-4581-90b7-3b4893690467-0', usage_metadata={'input_tokens': 48, 'output_tokens': 12, 'total_tokens': 60, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " '156')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await url_chatbot(\"Hello, I'm bob.\", \"https%3A%2F%2Ftechcrunch.com%2F2025%2F01%2F11%2Fi-got-soaked-driving-the-arc-sport-electric-boat%2F\", \"156\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AIMessage(content='Your name is Bob. How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 71, 'total_tokens': 87, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-34ebe027-9341-4a65-a5f1-8aaad2feab38-0', usage_metadata={'input_tokens': 71, 'output_tokens': 16, 'total_tokens': 87, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), '156')\n"
     ]
    }
   ],
   "source": [
    "print(await url_chatbot(\"What is my name?\", \"https%3A%2F%2Ftechcrunch.com%2F2025%2F01%2F11%2Fi-got-soaked-driving-the-arc-sport-electric-boat%2F\", \"156\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresCheckpointer = PostgresSaver(Connection.connect(db_url))\n",
    "url_graph = workflow.compile(checkpointer=postgresCheckpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile(checkpointer=postgresCheckpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = await conversation_history(\"0\", \"156\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hello, I'm bob.\", 'Hello Bob! How can I assist you today?', 'What is my name?', 'Your name is Bob. How can I help you today, Bob?']\n"
     ]
    }
   ],
   "source": [
    "print(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13928/982479204.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msnapshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "snapshot.values[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = snapshot.values[\"messages\"]\n",
    "    \n",
    "conversation_messages = [\n",
    "    message.content\n",
    "    for message in messages\n",
    "    if message.type in (\"human\", \"system\")\n",
    "    or (message.type == \"ai\" and not message.tool_calls)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello, I'm bob.\",\n",
       " 'Hello Bob! How can I assist you today?',\n",
       " 'What is my name?',\n",
       " 'Your name is Bob. How can I help you today, Bob?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastapi.testclient import TestClient\n",
    "\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_read():\n",
    "    response = client.get(\"/conversation_history/0/155\")\n",
    "    assert response.status_code == 200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
