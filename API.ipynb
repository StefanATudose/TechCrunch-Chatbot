{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import psycopg2\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"my_docs2\",\n",
    "    connection=\"postgresql+psycopg://stefan:gigelfrone112@localhost:5432/techvector\",\n",
    ")\n",
    "app = FastAPI()\n",
    "conn = psycopg2.connect(\"dbname=techvector user=stefan password=gigelfrone112 host=localhost port=5432\")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_date_objects(data):\n",
    "    \"\"\"\n",
    "    Recursively traverses the JSON object and replaces every dictionary\n",
    "    containing 'date' and 'type' keys with the value of the 'date' key.\n",
    "\n",
    "    :param data: JSON object (dict, list, or other types)\n",
    "    :return: Updated JSON object\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        # Check if the current dictionary is the one to replace\n",
    "        if \"date\" in data and \"type\" in data:\n",
    "            return data[\"date\"]\n",
    "        # Otherwise, process each key-value pair\n",
    "        return {key: replace_date_objects(value) for key, value in data.items()}\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        # Process each element in the list\n",
    "        return [replace_date_objects(item) for item in data]\n",
    "\n",
    "    # Return the data as is for other types\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Union\n",
    "\n",
    "from langchain_core.structured_query import (\n",
    "    Comparator,\n",
    "    Comparison,\n",
    "    Operation,\n",
    "    Operator,\n",
    "    StructuredQuery,\n",
    "    Visitor,\n",
    ")\n",
    "\n",
    "\n",
    "class CustomTranslator(Visitor):\n",
    "    \"\"\"Translate `PGVector` internal query language elements to valid filters.\"\"\"\n",
    "\n",
    "    allowed_operators = [Operator.AND, Operator.OR]\n",
    "    \"\"\"Subset of allowed logical operators.\"\"\"\n",
    "    allowed_comparators = [\n",
    "        Comparator.EQ,\n",
    "        Comparator.NE,\n",
    "        Comparator.GT,\n",
    "        Comparator.LT,\n",
    "        Comparator.IN,\n",
    "        Comparator.NIN,\n",
    "        Comparator.CONTAIN,\n",
    "        Comparator.LIKE,\n",
    "    ]\n",
    "    \"\"\"Subset of allowed logical comparators.\"\"\"\n",
    "\n",
    "    def _format_func(self, func: Union[Operator, Comparator]) -> str:\n",
    "        self._validate_func(func)\n",
    "        return f\"${func.value}\"\n",
    "\n",
    "    def visit_operation(self, operation: Operation) -> Dict:\n",
    "        args = [arg.accept(self) for arg in operation.arguments]\n",
    "        return {self._format_func(operation.operator): args}\n",
    "\n",
    "\n",
    "\n",
    "    def visit_comparison(self, comparison: Comparison) -> Dict:\n",
    "        return {\n",
    "            comparison.attribute: {\n",
    "                self._format_func(comparison.comparator): comparison.value\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def visit_structured_query(\n",
    "        self, structured_query: StructuredQuery\n",
    "    ) -> Tuple[str, dict]:\n",
    "        if structured_query.filter is None:\n",
    "            kwargs = {}\n",
    "        else:\n",
    "            kwargs = {\"filter\": structured_query.filter.accept(self)}\n",
    "            kwargs = replace_date_objects(kwargs)\n",
    "        return structured_query.query, kwargs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The title that the article was published under\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"author\",\n",
    "        description=\"The name of the author of the article\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"date\",\n",
    "        description=\"The date that the article was published on, in the format 'YYYY-MM-DD'. If the month is given by its name, it is converted to its number.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"category\",\n",
    "        description=\"The category that the article belongs to. One of ['AI', 'Apps', 'Biotech & Health', 'Climate', 'Commerce', 'Crypto', 'Enterprise', 'Fintech', 'Fundraising', 'Gadgets', 'Gaming', 'Government & Policy', 'Hardware', 'Media & Entertainment', 'Privacy', 'Robotics', 'Security', 'Social', 'Space', 'Startups', 'Transportation', 'Venture']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"url\",\n",
    "        description=\"The URL to the original TechCrunch article\",\n",
    "        type=\"link\",\n",
    "    )\n",
    "]\n",
    "document_content_description = \"The article content\"\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vector_store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    structured_query_translator=CustomTranslator(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/get_articles_by_query\")\n",
    "async def get_articles_by_query(query: str):\n",
    "    query = query.replace(\"'\", \"\\'\")\n",
    "    query = query.replace(\"’\", \"\\'\")\n",
    "    docs = retriever.invoke(query)\n",
    "    urls = list(set([doc.metadata[\"url\"] for doc in docs]))\n",
    "    cursor.execute(\"SELECT * FROM article WHERE link = ANY(%s);\", (urls,))\n",
    "    tuples = cursor.fetchall()\n",
    "    result_dict = [dict(zip(['url', 'title', 'time', 'img', 'category', 'summary', 'questions', 'author'], tup)) for tup in tuples]\n",
    "    return result_dict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/get_articles\")\n",
    "async def get_articles():\n",
    "    cursor.execute(\"SELECT * FROM article;\")\n",
    "    tuples = cursor.fetchall()\n",
    "    result_dict = [dict(zip(['url', 'title', 'time', 'img', 'category', 'summary', 'questions', 'author'], tup)) for tup in tuples]\n",
    "    return result_dict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/get_article\")\n",
    "async def get_article(url: str):\n",
    "    cursor.execute(f\"SELECT * FROM article where link = '{url}';\")\n",
    "    tuples = cursor.fetchone()\n",
    "    result_dict = dict(zip(['url', 'title', 'time', 'img', 'category', 'summary', 'questions', 'author'], tuples))\n",
    "    return result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://techcrunch.com/2025/01/09/nvidias-ai-avatar-sat-on-my-computer-screen-and-weirded-me-out/', 'title': 'Nvidia’s AI avatar sat on my computer screen and weirded me out', 'time': '4:31 PM PST · January 9, 2025', 'img': 'https://techcrunch.com/wp-content/uploads/2025/01/IMG_9CC69B31B7BF-1.jpeg?w=1024', 'category': 'AI', 'summary': 'Nvidia has introduced R2X, a prototype AI avatar designed to assist users directly from their desktop, combining advanced AI models with a human-like interface. While it can navigate apps, process files, and even observe users’ screens, early demos reveal some quirks, like odd facial expressions and occasional inaccuracies in its guidance. The company plans to open source R2X in 2025, potentially allowing developers to create personalized AI interactions. Despite its promise, the technology still faces challenges, hinting at both the excitement and the uncertainties of integrating AI into everyday computing.', 'questions': 'What specific features does R2X offer for assisting users with applications?&&&How does Nvidia plan to address the issues observed in the early demos of R2X?&&&What are the implications of Nvidia’s plans to open source the R2X avatars for developers and users?', 'author': 'Maxwell Zeff'}]\n"
     ]
    }
   ],
   "source": [
    "print(await get_articles_by_query('Give me an article about AI published on 9 January, 2025 by Maxwell Zeff, talking about nvidia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import InjectedToolArg, tool\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from copy import deepcopy\n",
    "from langchain_core.runnables import chain\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):#, url: Annotated[str, InjectedToolArg]) -> Tuple[str, list]:\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=3)#, filter={\"url\": {'$eq': url}})\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@chain\n",
    "def inject_user_id(ai_msg):\n",
    "    tool_calls = []\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        tool_call_copy = deepcopy(tool_call)\n",
    "        tool_call_copy[\"args\"][\"user_id\"] = user_id\n",
    "        tool_calls.append(tool_call_copy)\n",
    "    return tool_calls\n",
    "\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from psycopg import Connection\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "db_url = \"postgresql://stefan:gigelfrone112@localhost:5432/techvector\"\n",
    "\n",
    "postgresCheckpointer = PostgresSaver(Connection.connect(db_url))\n",
    "\n",
    "#postgresCheckpointer.setup()\n",
    "graph = graph_builder.compile(checkpointer=postgresCheckpointer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Do you still remember what I asked first?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you initially asked for the latest news about Elon Musk.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"Do you still remember what I asked first?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plan for next api:\n",
    "#chatbot on document splits of same article(fetched by article url)\n",
    "#q&a bot with memory\n",
    "\n",
    "@app.get(\"/continue_conversation\")\n",
    "async def continue_conversation(url: str, query: str):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT RELATED TO APIS: modifying metadata: from time, to just date and removed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_postgres.vectorstores._get_embedding_collection_store.<locals>.EmbeddingStore object at 0x718a3c111f00>\n",
      "1261\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from sqlalchemy.orm import Session\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings, collection_name=\"my_docs2\",\n",
    "    connection=\"postgresql+psycopg://stefan:gigelfrone112@localhost:5432/techvector\", use_jsonb=True)\n",
    "\n",
    "with Session(vectorstore.session_maker.bind) as session:\n",
    "    docs = session.query(vectorstore.EmbeddingStore).all()\n",
    "\n",
    "print(docs[0])\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://techcrunch.com/2025/01/01/2024-the-year-silicon-valley-stifled-the-ai-doom-movement/', 'date': '2025-01-01', 'title': 'Silicon Valley stifled the AI doom movement in 2024', 'author': 'Maxwell Zeff', 'category': 'AI', 'start_index': 11322}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].cmetadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifying metadata for time to just date\n",
    "\n",
    "from multiprocessing import connection\n",
    "\n",
    "date_conversion = {'January': '1', 'February': '2', 'March': '3', 'April': '4', 'May': '5', 'June': '6', 'July': '7', 'August': '8', 'September': '9', 'October': '10', 'November': '11', 'December': '12'}\n",
    "\n",
    "\n",
    "original_metadata = [doc.cmetadata for doc in docs]\n",
    "for doc in original_metadata:\n",
    "    # time = doc['time']\n",
    "    # time = time.replace(',', '')\n",
    "    # time = time.split(' ')\n",
    "    # time = f\"{time[2]}-{date_conversion[time[0]].zfill(2)}-{time[1].zfill(2)}\"\n",
    "    # doc['time'] = time\n",
    "    doc['date'] = doc['time']['date'] #{'date': doc['time']['date'], 'type': 'date'}\n",
    "    doc.pop('time')\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "texts = [doc.document for doc in docs]\n",
    "embeddings = [doc.embedding for doc in docs]\n",
    "text_embedding = list(zip(texts, embeddings))\n",
    "vector_store = PGVector.from_embeddings(text_embeddings=text_embedding, \n",
    "                                        embedding=embeddings,\n",
    "                                        metadatas=original_metadata,\n",
    "                                        connection=\"postgresql+psycopg://stefan:gigelfrone112@localhost:5432/techvector\",\n",
    "                                        collection_name=\"my_docs2\",\n",
    "                                        pre_delete_collection=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1261\n"
     ]
    }
   ],
   "source": [
    "ids_to_delete = [doc.id for doc in docs if \"image\" in doc.cmetadata]\n",
    "print(len(ids_to_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.delete(ids_to_delete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
